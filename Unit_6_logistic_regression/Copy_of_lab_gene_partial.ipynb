{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Copy of lab_gene_partial.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4b18chLK5E6"
      },
      "source": [
        "# Lab:  Logistic Regression for Gene Expression Data\n",
        "\n",
        "In this lab, we use logistic regression to predict biological characteristics (\"phenotypes\") from gene expression data.  In addition to the concepts in [breast cancer demo](./breast_cancer.ipynb), you will learn to:\n",
        "* Handle missing data\n",
        "* Perform multi-class logistic classification\n",
        "* Create a confusion matrix\n",
        "* Use L1-regularization for improved estimation in the case of sparse weights (Grad students only)\n",
        "\n",
        "## Background\n",
        "\n",
        "Genes are the basic unit in the DNA and encode blueprints for proteins.  When proteins are synthesized from a gene, the gene is said to \"express\".  Micro-arrays are devices that measure the expression levels of large numbers of genes in parallel.  By finding correlations between expression levels and phenotypes, scientists can identify possible genetic markers for biological characteristics.\n",
        "\n",
        "The data in this lab comes from:\n",
        "\n",
        "https://archive.ics.uci.edu/ml/datasets/Mice+Protein+Expression\n",
        "\n",
        "In this data, mice were characterized by three properties:\n",
        "* Whether they had down's syndrome (trisomy) or not\n",
        "* Whether they were stimulated to learn or not\n",
        "* Whether they had a drug memantine or a saline control solution.\n",
        "\n",
        "With these three choices, there are 8 possible classes for each mouse.  For each mouse, the expression levels were measured across 77 genes.  We will see if the characteristics can be predicted from the gene expression levels.  This classification could reveal which genes are potentially involved in Down's syndrome and if drugs and learning have any noticeable effects.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGV2hdn6K5E8"
      },
      "source": [
        "## Load the Data\n",
        "\n",
        "We begin by loading the standard modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xAcxbDdK5E9"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn import linear_model, preprocessing"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3p9lTmqK5FD"
      },
      "source": [
        "Use the `pd.read_excel` command to read the data from \n",
        "\n",
        "https://archive.ics.uci.edu/ml/machine-learning-databases/00342/Data_Cortex_Nuclear.xls\n",
        "\n",
        "into a dataframe `df`.  Use the `index_col` option to specify that column 0 is the index.  Use the `df.head()` to print the first few rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7gQc-nhK5FE",
        "outputId": "014a9403-b5f1-4377-d556-151537db1327",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        }
      },
      "source": [
        "# TODO\n",
        "#   df = pd.read_excel(...)\n",
        "df = pd.read_excel('https://archive.ics.uci.edu/ml/machine-learning-databases/00342/Data_Cortex_Nuclear.xls', index_col=0)\n",
        "df.head(6)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DYRK1A_N</th>\n",
              "      <th>ITSN1_N</th>\n",
              "      <th>BDNF_N</th>\n",
              "      <th>NR1_N</th>\n",
              "      <th>NR2A_N</th>\n",
              "      <th>pAKT_N</th>\n",
              "      <th>pBRAF_N</th>\n",
              "      <th>pCAMKII_N</th>\n",
              "      <th>pCREB_N</th>\n",
              "      <th>pELK_N</th>\n",
              "      <th>pERK_N</th>\n",
              "      <th>pJNK_N</th>\n",
              "      <th>PKCA_N</th>\n",
              "      <th>pMEK_N</th>\n",
              "      <th>pNR1_N</th>\n",
              "      <th>pNR2A_N</th>\n",
              "      <th>pNR2B_N</th>\n",
              "      <th>pPKCAB_N</th>\n",
              "      <th>pRSK_N</th>\n",
              "      <th>AKT_N</th>\n",
              "      <th>BRAF_N</th>\n",
              "      <th>CAMKII_N</th>\n",
              "      <th>CREB_N</th>\n",
              "      <th>ELK_N</th>\n",
              "      <th>ERK_N</th>\n",
              "      <th>GSK3B_N</th>\n",
              "      <th>JNK_N</th>\n",
              "      <th>MEK_N</th>\n",
              "      <th>TRKA_N</th>\n",
              "      <th>RSK_N</th>\n",
              "      <th>APP_N</th>\n",
              "      <th>Bcatenin_N</th>\n",
              "      <th>SOD1_N</th>\n",
              "      <th>MTOR_N</th>\n",
              "      <th>P38_N</th>\n",
              "      <th>pMTOR_N</th>\n",
              "      <th>DSCR1_N</th>\n",
              "      <th>AMPKA_N</th>\n",
              "      <th>NR2B_N</th>\n",
              "      <th>pNUMB_N</th>\n",
              "      <th>...</th>\n",
              "      <th>TIAM1_N</th>\n",
              "      <th>pP70S6_N</th>\n",
              "      <th>NUMB_N</th>\n",
              "      <th>P70S6_N</th>\n",
              "      <th>pGSK3B_N</th>\n",
              "      <th>pPKCG_N</th>\n",
              "      <th>CDK5_N</th>\n",
              "      <th>S6_N</th>\n",
              "      <th>ADARB1_N</th>\n",
              "      <th>AcetylH3K9_N</th>\n",
              "      <th>RRP1_N</th>\n",
              "      <th>BAX_N</th>\n",
              "      <th>ARC_N</th>\n",
              "      <th>ERBB4_N</th>\n",
              "      <th>nNOS_N</th>\n",
              "      <th>Tau_N</th>\n",
              "      <th>GFAP_N</th>\n",
              "      <th>GluR3_N</th>\n",
              "      <th>GluR4_N</th>\n",
              "      <th>IL1B_N</th>\n",
              "      <th>P3525_N</th>\n",
              "      <th>pCASP9_N</th>\n",
              "      <th>PSD95_N</th>\n",
              "      <th>SNCA_N</th>\n",
              "      <th>Ubiquitin_N</th>\n",
              "      <th>pGSK3B_Tyr216_N</th>\n",
              "      <th>SHH_N</th>\n",
              "      <th>BAD_N</th>\n",
              "      <th>BCL2_N</th>\n",
              "      <th>pS6_N</th>\n",
              "      <th>pCFOS_N</th>\n",
              "      <th>SYP_N</th>\n",
              "      <th>H3AcK18_N</th>\n",
              "      <th>EGR1_N</th>\n",
              "      <th>H3MeK4_N</th>\n",
              "      <th>CaNA_N</th>\n",
              "      <th>Genotype</th>\n",
              "      <th>Treatment</th>\n",
              "      <th>Behavior</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MouseID</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>309_1</th>\n",
              "      <td>0.503644</td>\n",
              "      <td>0.747193</td>\n",
              "      <td>0.430175</td>\n",
              "      <td>2.816329</td>\n",
              "      <td>5.990152</td>\n",
              "      <td>0.218830</td>\n",
              "      <td>0.177565</td>\n",
              "      <td>2.373744</td>\n",
              "      <td>0.232224</td>\n",
              "      <td>1.750936</td>\n",
              "      <td>0.687906</td>\n",
              "      <td>0.306382</td>\n",
              "      <td>0.402698</td>\n",
              "      <td>0.296927</td>\n",
              "      <td>1.022060</td>\n",
              "      <td>0.605673</td>\n",
              "      <td>1.877684</td>\n",
              "      <td>2.308745</td>\n",
              "      <td>0.441599</td>\n",
              "      <td>0.859366</td>\n",
              "      <td>0.416289</td>\n",
              "      <td>0.369608</td>\n",
              "      <td>0.178944</td>\n",
              "      <td>1.866358</td>\n",
              "      <td>3.685247</td>\n",
              "      <td>1.537227</td>\n",
              "      <td>0.264526</td>\n",
              "      <td>0.319677</td>\n",
              "      <td>0.813866</td>\n",
              "      <td>0.165846</td>\n",
              "      <td>0.453910</td>\n",
              "      <td>3.037621</td>\n",
              "      <td>0.369510</td>\n",
              "      <td>0.458539</td>\n",
              "      <td>0.335336</td>\n",
              "      <td>0.825192</td>\n",
              "      <td>0.576916</td>\n",
              "      <td>0.448099</td>\n",
              "      <td>0.586271</td>\n",
              "      <td>0.394721</td>\n",
              "      <td>...</td>\n",
              "      <td>0.482864</td>\n",
              "      <td>0.294170</td>\n",
              "      <td>0.182150</td>\n",
              "      <td>0.842725</td>\n",
              "      <td>0.192608</td>\n",
              "      <td>1.443091</td>\n",
              "      <td>0.294700</td>\n",
              "      <td>0.354605</td>\n",
              "      <td>1.339070</td>\n",
              "      <td>0.170119</td>\n",
              "      <td>0.159102</td>\n",
              "      <td>0.188852</td>\n",
              "      <td>0.106305</td>\n",
              "      <td>0.144989</td>\n",
              "      <td>0.176668</td>\n",
              "      <td>0.125190</td>\n",
              "      <td>0.115291</td>\n",
              "      <td>0.228043</td>\n",
              "      <td>0.142756</td>\n",
              "      <td>0.430957</td>\n",
              "      <td>0.247538</td>\n",
              "      <td>1.603310</td>\n",
              "      <td>2.014875</td>\n",
              "      <td>0.108234</td>\n",
              "      <td>1.044979</td>\n",
              "      <td>0.831557</td>\n",
              "      <td>0.188852</td>\n",
              "      <td>0.122652</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.106305</td>\n",
              "      <td>0.108336</td>\n",
              "      <td>0.427099</td>\n",
              "      <td>0.114783</td>\n",
              "      <td>0.131790</td>\n",
              "      <td>0.128186</td>\n",
              "      <td>1.675652</td>\n",
              "      <td>Control</td>\n",
              "      <td>Memantine</td>\n",
              "      <td>C/S</td>\n",
              "      <td>c-CS-m</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309_2</th>\n",
              "      <td>0.514617</td>\n",
              "      <td>0.689064</td>\n",
              "      <td>0.411770</td>\n",
              "      <td>2.789514</td>\n",
              "      <td>5.685038</td>\n",
              "      <td>0.211636</td>\n",
              "      <td>0.172817</td>\n",
              "      <td>2.292150</td>\n",
              "      <td>0.226972</td>\n",
              "      <td>1.596377</td>\n",
              "      <td>0.695006</td>\n",
              "      <td>0.299051</td>\n",
              "      <td>0.385987</td>\n",
              "      <td>0.281319</td>\n",
              "      <td>0.956676</td>\n",
              "      <td>0.587559</td>\n",
              "      <td>1.725774</td>\n",
              "      <td>2.043037</td>\n",
              "      <td>0.445222</td>\n",
              "      <td>0.834659</td>\n",
              "      <td>0.400364</td>\n",
              "      <td>0.356178</td>\n",
              "      <td>0.173680</td>\n",
              "      <td>1.761047</td>\n",
              "      <td>3.485287</td>\n",
              "      <td>1.509249</td>\n",
              "      <td>0.255727</td>\n",
              "      <td>0.304419</td>\n",
              "      <td>0.780504</td>\n",
              "      <td>0.157194</td>\n",
              "      <td>0.430940</td>\n",
              "      <td>2.921882</td>\n",
              "      <td>0.342279</td>\n",
              "      <td>0.423560</td>\n",
              "      <td>0.324835</td>\n",
              "      <td>0.761718</td>\n",
              "      <td>0.545097</td>\n",
              "      <td>0.420876</td>\n",
              "      <td>0.545097</td>\n",
              "      <td>0.368255</td>\n",
              "      <td>...</td>\n",
              "      <td>0.454519</td>\n",
              "      <td>0.276431</td>\n",
              "      <td>0.182086</td>\n",
              "      <td>0.847615</td>\n",
              "      <td>0.194815</td>\n",
              "      <td>1.439460</td>\n",
              "      <td>0.294060</td>\n",
              "      <td>0.354548</td>\n",
              "      <td>1.306323</td>\n",
              "      <td>0.171427</td>\n",
              "      <td>0.158129</td>\n",
              "      <td>0.184570</td>\n",
              "      <td>0.106592</td>\n",
              "      <td>0.150471</td>\n",
              "      <td>0.178309</td>\n",
              "      <td>0.134275</td>\n",
              "      <td>0.118235</td>\n",
              "      <td>0.238073</td>\n",
              "      <td>0.142037</td>\n",
              "      <td>0.457156</td>\n",
              "      <td>0.257632</td>\n",
              "      <td>1.671738</td>\n",
              "      <td>2.004605</td>\n",
              "      <td>0.109749</td>\n",
              "      <td>1.009883</td>\n",
              "      <td>0.849270</td>\n",
              "      <td>0.200404</td>\n",
              "      <td>0.116682</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.106592</td>\n",
              "      <td>0.104315</td>\n",
              "      <td>0.441581</td>\n",
              "      <td>0.111974</td>\n",
              "      <td>0.135103</td>\n",
              "      <td>0.131119</td>\n",
              "      <td>1.743610</td>\n",
              "      <td>Control</td>\n",
              "      <td>Memantine</td>\n",
              "      <td>C/S</td>\n",
              "      <td>c-CS-m</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309_3</th>\n",
              "      <td>0.509183</td>\n",
              "      <td>0.730247</td>\n",
              "      <td>0.418309</td>\n",
              "      <td>2.687201</td>\n",
              "      <td>5.622059</td>\n",
              "      <td>0.209011</td>\n",
              "      <td>0.175722</td>\n",
              "      <td>2.283337</td>\n",
              "      <td>0.230247</td>\n",
              "      <td>1.561316</td>\n",
              "      <td>0.677348</td>\n",
              "      <td>0.291276</td>\n",
              "      <td>0.381002</td>\n",
              "      <td>0.281710</td>\n",
              "      <td>1.003635</td>\n",
              "      <td>0.602449</td>\n",
              "      <td>1.731873</td>\n",
              "      <td>2.017984</td>\n",
              "      <td>0.467668</td>\n",
              "      <td>0.814329</td>\n",
              "      <td>0.399847</td>\n",
              "      <td>0.368089</td>\n",
              "      <td>0.173905</td>\n",
              "      <td>1.765544</td>\n",
              "      <td>3.571456</td>\n",
              "      <td>1.501244</td>\n",
              "      <td>0.259614</td>\n",
              "      <td>0.311747</td>\n",
              "      <td>0.785154</td>\n",
              "      <td>0.160895</td>\n",
              "      <td>0.423187</td>\n",
              "      <td>2.944136</td>\n",
              "      <td>0.343696</td>\n",
              "      <td>0.425005</td>\n",
              "      <td>0.324852</td>\n",
              "      <td>0.757031</td>\n",
              "      <td>0.543620</td>\n",
              "      <td>0.404630</td>\n",
              "      <td>0.552994</td>\n",
              "      <td>0.363880</td>\n",
              "      <td>...</td>\n",
              "      <td>0.447197</td>\n",
              "      <td>0.256648</td>\n",
              "      <td>0.184388</td>\n",
              "      <td>0.856166</td>\n",
              "      <td>0.200737</td>\n",
              "      <td>1.524364</td>\n",
              "      <td>0.301881</td>\n",
              "      <td>0.386087</td>\n",
              "      <td>1.279600</td>\n",
              "      <td>0.185456</td>\n",
              "      <td>0.148696</td>\n",
              "      <td>0.190532</td>\n",
              "      <td>0.108303</td>\n",
              "      <td>0.145330</td>\n",
              "      <td>0.176213</td>\n",
              "      <td>0.132560</td>\n",
              "      <td>0.117760</td>\n",
              "      <td>0.244817</td>\n",
              "      <td>0.142445</td>\n",
              "      <td>0.510472</td>\n",
              "      <td>0.255343</td>\n",
              "      <td>1.663550</td>\n",
              "      <td>2.016831</td>\n",
              "      <td>0.108196</td>\n",
              "      <td>0.996848</td>\n",
              "      <td>0.846709</td>\n",
              "      <td>0.193685</td>\n",
              "      <td>0.118508</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.108303</td>\n",
              "      <td>0.106219</td>\n",
              "      <td>0.435777</td>\n",
              "      <td>0.111883</td>\n",
              "      <td>0.133362</td>\n",
              "      <td>0.127431</td>\n",
              "      <td>1.926427</td>\n",
              "      <td>Control</td>\n",
              "      <td>Memantine</td>\n",
              "      <td>C/S</td>\n",
              "      <td>c-CS-m</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309_4</th>\n",
              "      <td>0.442107</td>\n",
              "      <td>0.617076</td>\n",
              "      <td>0.358626</td>\n",
              "      <td>2.466947</td>\n",
              "      <td>4.979503</td>\n",
              "      <td>0.222886</td>\n",
              "      <td>0.176463</td>\n",
              "      <td>2.152301</td>\n",
              "      <td>0.207004</td>\n",
              "      <td>1.595086</td>\n",
              "      <td>0.583277</td>\n",
              "      <td>0.296729</td>\n",
              "      <td>0.377087</td>\n",
              "      <td>0.313832</td>\n",
              "      <td>0.875390</td>\n",
              "      <td>0.520293</td>\n",
              "      <td>1.566852</td>\n",
              "      <td>2.132754</td>\n",
              "      <td>0.477671</td>\n",
              "      <td>0.727705</td>\n",
              "      <td>0.385639</td>\n",
              "      <td>0.362970</td>\n",
              "      <td>0.179449</td>\n",
              "      <td>1.286277</td>\n",
              "      <td>2.970137</td>\n",
              "      <td>1.419710</td>\n",
              "      <td>0.259536</td>\n",
              "      <td>0.279218</td>\n",
              "      <td>0.734492</td>\n",
              "      <td>0.162210</td>\n",
              "      <td>0.410615</td>\n",
              "      <td>2.500204</td>\n",
              "      <td>0.344509</td>\n",
              "      <td>0.429211</td>\n",
              "      <td>0.330121</td>\n",
              "      <td>0.746980</td>\n",
              "      <td>0.546763</td>\n",
              "      <td>0.386860</td>\n",
              "      <td>0.547849</td>\n",
              "      <td>0.366771</td>\n",
              "      <td>...</td>\n",
              "      <td>0.442650</td>\n",
              "      <td>0.398534</td>\n",
              "      <td>0.161768</td>\n",
              "      <td>0.760234</td>\n",
              "      <td>0.184169</td>\n",
              "      <td>1.612382</td>\n",
              "      <td>0.296382</td>\n",
              "      <td>0.290680</td>\n",
              "      <td>1.198765</td>\n",
              "      <td>0.159799</td>\n",
              "      <td>0.166112</td>\n",
              "      <td>0.185323</td>\n",
              "      <td>0.103184</td>\n",
              "      <td>0.140656</td>\n",
              "      <td>0.163804</td>\n",
              "      <td>0.123210</td>\n",
              "      <td>0.117439</td>\n",
              "      <td>0.234947</td>\n",
              "      <td>0.145068</td>\n",
              "      <td>0.430996</td>\n",
              "      <td>0.251103</td>\n",
              "      <td>1.484624</td>\n",
              "      <td>1.957233</td>\n",
              "      <td>0.119883</td>\n",
              "      <td>0.990225</td>\n",
              "      <td>0.833277</td>\n",
              "      <td>0.192112</td>\n",
              "      <td>0.132781</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.103184</td>\n",
              "      <td>0.111262</td>\n",
              "      <td>0.391691</td>\n",
              "      <td>0.130405</td>\n",
              "      <td>0.147444</td>\n",
              "      <td>0.146901</td>\n",
              "      <td>1.700563</td>\n",
              "      <td>Control</td>\n",
              "      <td>Memantine</td>\n",
              "      <td>C/S</td>\n",
              "      <td>c-CS-m</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309_5</th>\n",
              "      <td>0.434940</td>\n",
              "      <td>0.617430</td>\n",
              "      <td>0.358802</td>\n",
              "      <td>2.365785</td>\n",
              "      <td>4.718679</td>\n",
              "      <td>0.213106</td>\n",
              "      <td>0.173627</td>\n",
              "      <td>2.134014</td>\n",
              "      <td>0.192158</td>\n",
              "      <td>1.504230</td>\n",
              "      <td>0.550960</td>\n",
              "      <td>0.286961</td>\n",
              "      <td>0.363502</td>\n",
              "      <td>0.277964</td>\n",
              "      <td>0.864912</td>\n",
              "      <td>0.507990</td>\n",
              "      <td>1.480059</td>\n",
              "      <td>2.013697</td>\n",
              "      <td>0.483416</td>\n",
              "      <td>0.687794</td>\n",
              "      <td>0.367531</td>\n",
              "      <td>0.355311</td>\n",
              "      <td>0.174836</td>\n",
              "      <td>1.324695</td>\n",
              "      <td>2.896334</td>\n",
              "      <td>1.359876</td>\n",
              "      <td>0.250705</td>\n",
              "      <td>0.273667</td>\n",
              "      <td>0.702699</td>\n",
              "      <td>0.154827</td>\n",
              "      <td>0.398550</td>\n",
              "      <td>2.456560</td>\n",
              "      <td>0.329126</td>\n",
              "      <td>0.408755</td>\n",
              "      <td>0.313415</td>\n",
              "      <td>0.691956</td>\n",
              "      <td>0.536860</td>\n",
              "      <td>0.360816</td>\n",
              "      <td>0.512824</td>\n",
              "      <td>0.351551</td>\n",
              "      <td>...</td>\n",
              "      <td>0.419095</td>\n",
              "      <td>0.393447</td>\n",
              "      <td>0.160200</td>\n",
              "      <td>0.768113</td>\n",
              "      <td>0.185718</td>\n",
              "      <td>1.645807</td>\n",
              "      <td>0.296829</td>\n",
              "      <td>0.309345</td>\n",
              "      <td>1.206995</td>\n",
              "      <td>0.164650</td>\n",
              "      <td>0.160687</td>\n",
              "      <td>0.188221</td>\n",
              "      <td>0.104784</td>\n",
              "      <td>0.141983</td>\n",
              "      <td>0.167710</td>\n",
              "      <td>0.136838</td>\n",
              "      <td>0.116048</td>\n",
              "      <td>0.255528</td>\n",
              "      <td>0.140871</td>\n",
              "      <td>0.481227</td>\n",
              "      <td>0.251773</td>\n",
              "      <td>1.534835</td>\n",
              "      <td>2.009109</td>\n",
              "      <td>0.119524</td>\n",
              "      <td>0.997775</td>\n",
              "      <td>0.878668</td>\n",
              "      <td>0.205604</td>\n",
              "      <td>0.129954</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.104784</td>\n",
              "      <td>0.110694</td>\n",
              "      <td>0.434154</td>\n",
              "      <td>0.118481</td>\n",
              "      <td>0.140314</td>\n",
              "      <td>0.148380</td>\n",
              "      <td>1.839730</td>\n",
              "      <td>Control</td>\n",
              "      <td>Memantine</td>\n",
              "      <td>C/S</td>\n",
              "      <td>c-CS-m</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309_6</th>\n",
              "      <td>0.447506</td>\n",
              "      <td>0.628176</td>\n",
              "      <td>0.367388</td>\n",
              "      <td>2.385939</td>\n",
              "      <td>4.807635</td>\n",
              "      <td>0.218578</td>\n",
              "      <td>0.176233</td>\n",
              "      <td>2.141282</td>\n",
              "      <td>0.195188</td>\n",
              "      <td>1.442398</td>\n",
              "      <td>0.566340</td>\n",
              "      <td>0.289824</td>\n",
              "      <td>0.363893</td>\n",
              "      <td>0.266837</td>\n",
              "      <td>0.859121</td>\n",
              "      <td>0.521307</td>\n",
              "      <td>1.538244</td>\n",
              "      <td>1.968275</td>\n",
              "      <td>0.495900</td>\n",
              "      <td>0.672402</td>\n",
              "      <td>0.369404</td>\n",
              "      <td>0.357172</td>\n",
              "      <td>0.179728</td>\n",
              "      <td>1.227450</td>\n",
              "      <td>2.956983</td>\n",
              "      <td>1.447910</td>\n",
              "      <td>0.250840</td>\n",
              "      <td>0.284044</td>\n",
              "      <td>0.704396</td>\n",
              "      <td>0.156876</td>\n",
              "      <td>0.391047</td>\n",
              "      <td>2.467133</td>\n",
              "      <td>0.327598</td>\n",
              "      <td>0.404490</td>\n",
              "      <td>0.296276</td>\n",
              "      <td>0.674419</td>\n",
              "      <td>0.539723</td>\n",
              "      <td>0.354214</td>\n",
              "      <td>0.514316</td>\n",
              "      <td>0.347224</td>\n",
              "      <td>...</td>\n",
              "      <td>0.412824</td>\n",
              "      <td>0.382578</td>\n",
              "      <td>0.162330</td>\n",
              "      <td>0.779695</td>\n",
              "      <td>0.186793</td>\n",
              "      <td>1.634615</td>\n",
              "      <td>0.288037</td>\n",
              "      <td>0.332367</td>\n",
              "      <td>1.123445</td>\n",
              "      <td>0.175693</td>\n",
              "      <td>0.150594</td>\n",
              "      <td>0.183824</td>\n",
              "      <td>0.106476</td>\n",
              "      <td>0.139564</td>\n",
              "      <td>0.174844</td>\n",
              "      <td>0.130515</td>\n",
              "      <td>0.115243</td>\n",
              "      <td>0.236850</td>\n",
              "      <td>0.136454</td>\n",
              "      <td>0.478577</td>\n",
              "      <td>0.244485</td>\n",
              "      <td>1.507777</td>\n",
              "      <td>2.003535</td>\n",
              "      <td>0.120687</td>\n",
              "      <td>0.920178</td>\n",
              "      <td>0.843679</td>\n",
              "      <td>0.190469</td>\n",
              "      <td>0.131575</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.106476</td>\n",
              "      <td>0.109446</td>\n",
              "      <td>0.439833</td>\n",
              "      <td>0.116657</td>\n",
              "      <td>0.140766</td>\n",
              "      <td>0.142180</td>\n",
              "      <td>1.816389</td>\n",
              "      <td>Control</td>\n",
              "      <td>Memantine</td>\n",
              "      <td>C/S</td>\n",
              "      <td>c-CS-m</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6 rows × 81 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         DYRK1A_N   ITSN1_N    BDNF_N  ...  Treatment  Behavior   class\n",
              "MouseID                                ...                             \n",
              "309_1    0.503644  0.747193  0.430175  ...  Memantine       C/S  c-CS-m\n",
              "309_2    0.514617  0.689064  0.411770  ...  Memantine       C/S  c-CS-m\n",
              "309_3    0.509183  0.730247  0.418309  ...  Memantine       C/S  c-CS-m\n",
              "309_4    0.442107  0.617076  0.358626  ...  Memantine       C/S  c-CS-m\n",
              "309_5    0.434940  0.617430  0.358802  ...  Memantine       C/S  c-CS-m\n",
              "309_6    0.447506  0.628176  0.367388  ...  Memantine       C/S  c-CS-m\n",
              "\n",
              "[6 rows x 81 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCi0BKNxe3hM",
        "outputId": "1977d52e-151d-43dc-fc54-e14ce3944343",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1080, 81)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5RTPUdlK5FI"
      },
      "source": [
        "This data has missing values.  The site:\n",
        "\n",
        "http://pandas.pydata.org/pandas-docs/stable/missing_data.html\n",
        "\n",
        "has an excellent summary of methods to deal with missing values.  Following the techniques there, create a new data frame `df1` where the missing values in each column are filled with the mean values from the non-missing values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZScHEg3K5FJ",
        "outputId": "ac7eb87e-fc1f-4865-e3ef-ad9ce6ad3ee7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        }
      },
      "source": [
        "# TODO\n",
        "#  df1 = ...\n",
        "df1 = df.where(pd.notna(df), df.mean(), axis='columns')\n",
        "df1.head(6)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DYRK1A_N</th>\n",
              "      <th>ITSN1_N</th>\n",
              "      <th>BDNF_N</th>\n",
              "      <th>NR1_N</th>\n",
              "      <th>NR2A_N</th>\n",
              "      <th>pAKT_N</th>\n",
              "      <th>pBRAF_N</th>\n",
              "      <th>pCAMKII_N</th>\n",
              "      <th>pCREB_N</th>\n",
              "      <th>pELK_N</th>\n",
              "      <th>pERK_N</th>\n",
              "      <th>pJNK_N</th>\n",
              "      <th>PKCA_N</th>\n",
              "      <th>pMEK_N</th>\n",
              "      <th>pNR1_N</th>\n",
              "      <th>pNR2A_N</th>\n",
              "      <th>pNR2B_N</th>\n",
              "      <th>pPKCAB_N</th>\n",
              "      <th>pRSK_N</th>\n",
              "      <th>AKT_N</th>\n",
              "      <th>BRAF_N</th>\n",
              "      <th>CAMKII_N</th>\n",
              "      <th>CREB_N</th>\n",
              "      <th>ELK_N</th>\n",
              "      <th>ERK_N</th>\n",
              "      <th>GSK3B_N</th>\n",
              "      <th>JNK_N</th>\n",
              "      <th>MEK_N</th>\n",
              "      <th>TRKA_N</th>\n",
              "      <th>RSK_N</th>\n",
              "      <th>APP_N</th>\n",
              "      <th>Bcatenin_N</th>\n",
              "      <th>SOD1_N</th>\n",
              "      <th>MTOR_N</th>\n",
              "      <th>P38_N</th>\n",
              "      <th>pMTOR_N</th>\n",
              "      <th>DSCR1_N</th>\n",
              "      <th>AMPKA_N</th>\n",
              "      <th>NR2B_N</th>\n",
              "      <th>pNUMB_N</th>\n",
              "      <th>...</th>\n",
              "      <th>TIAM1_N</th>\n",
              "      <th>pP70S6_N</th>\n",
              "      <th>NUMB_N</th>\n",
              "      <th>P70S6_N</th>\n",
              "      <th>pGSK3B_N</th>\n",
              "      <th>pPKCG_N</th>\n",
              "      <th>CDK5_N</th>\n",
              "      <th>S6_N</th>\n",
              "      <th>ADARB1_N</th>\n",
              "      <th>AcetylH3K9_N</th>\n",
              "      <th>RRP1_N</th>\n",
              "      <th>BAX_N</th>\n",
              "      <th>ARC_N</th>\n",
              "      <th>ERBB4_N</th>\n",
              "      <th>nNOS_N</th>\n",
              "      <th>Tau_N</th>\n",
              "      <th>GFAP_N</th>\n",
              "      <th>GluR3_N</th>\n",
              "      <th>GluR4_N</th>\n",
              "      <th>IL1B_N</th>\n",
              "      <th>P3525_N</th>\n",
              "      <th>pCASP9_N</th>\n",
              "      <th>PSD95_N</th>\n",
              "      <th>SNCA_N</th>\n",
              "      <th>Ubiquitin_N</th>\n",
              "      <th>pGSK3B_Tyr216_N</th>\n",
              "      <th>SHH_N</th>\n",
              "      <th>BAD_N</th>\n",
              "      <th>BCL2_N</th>\n",
              "      <th>pS6_N</th>\n",
              "      <th>pCFOS_N</th>\n",
              "      <th>SYP_N</th>\n",
              "      <th>H3AcK18_N</th>\n",
              "      <th>EGR1_N</th>\n",
              "      <th>H3MeK4_N</th>\n",
              "      <th>CaNA_N</th>\n",
              "      <th>Genotype</th>\n",
              "      <th>Treatment</th>\n",
              "      <th>Behavior</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MouseID</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>309_1</th>\n",
              "      <td>0.503644</td>\n",
              "      <td>0.747193</td>\n",
              "      <td>0.430175</td>\n",
              "      <td>2.816329</td>\n",
              "      <td>5.990152</td>\n",
              "      <td>0.218830</td>\n",
              "      <td>0.177565</td>\n",
              "      <td>2.373744</td>\n",
              "      <td>0.232224</td>\n",
              "      <td>1.750936</td>\n",
              "      <td>0.687906</td>\n",
              "      <td>0.306382</td>\n",
              "      <td>0.402698</td>\n",
              "      <td>0.296927</td>\n",
              "      <td>1.022060</td>\n",
              "      <td>0.605673</td>\n",
              "      <td>1.877684</td>\n",
              "      <td>2.308745</td>\n",
              "      <td>0.441599</td>\n",
              "      <td>0.859366</td>\n",
              "      <td>0.416289</td>\n",
              "      <td>0.369608</td>\n",
              "      <td>0.178944</td>\n",
              "      <td>1.866358</td>\n",
              "      <td>3.685247</td>\n",
              "      <td>1.537227</td>\n",
              "      <td>0.264526</td>\n",
              "      <td>0.319677</td>\n",
              "      <td>0.813866</td>\n",
              "      <td>0.165846</td>\n",
              "      <td>0.453910</td>\n",
              "      <td>3.037621</td>\n",
              "      <td>0.369510</td>\n",
              "      <td>0.458539</td>\n",
              "      <td>0.335336</td>\n",
              "      <td>0.825192</td>\n",
              "      <td>0.576916</td>\n",
              "      <td>0.448099</td>\n",
              "      <td>0.586271</td>\n",
              "      <td>0.394721</td>\n",
              "      <td>...</td>\n",
              "      <td>0.482864</td>\n",
              "      <td>0.294170</td>\n",
              "      <td>0.182150</td>\n",
              "      <td>0.842725</td>\n",
              "      <td>0.192608</td>\n",
              "      <td>1.443091</td>\n",
              "      <td>0.294700</td>\n",
              "      <td>0.354605</td>\n",
              "      <td>1.339070</td>\n",
              "      <td>0.170119</td>\n",
              "      <td>0.159102</td>\n",
              "      <td>0.188852</td>\n",
              "      <td>0.106305</td>\n",
              "      <td>0.144989</td>\n",
              "      <td>0.176668</td>\n",
              "      <td>0.125190</td>\n",
              "      <td>0.115291</td>\n",
              "      <td>0.228043</td>\n",
              "      <td>0.142756</td>\n",
              "      <td>0.430957</td>\n",
              "      <td>0.247538</td>\n",
              "      <td>1.603310</td>\n",
              "      <td>2.014875</td>\n",
              "      <td>0.108234</td>\n",
              "      <td>1.044979</td>\n",
              "      <td>0.831557</td>\n",
              "      <td>0.188852</td>\n",
              "      <td>0.122652</td>\n",
              "      <td>0.134762</td>\n",
              "      <td>0.106305</td>\n",
              "      <td>0.108336</td>\n",
              "      <td>0.427099</td>\n",
              "      <td>0.114783</td>\n",
              "      <td>0.131790</td>\n",
              "      <td>0.128186</td>\n",
              "      <td>1.675652</td>\n",
              "      <td>Control</td>\n",
              "      <td>Memantine</td>\n",
              "      <td>C/S</td>\n",
              "      <td>c-CS-m</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309_2</th>\n",
              "      <td>0.514617</td>\n",
              "      <td>0.689064</td>\n",
              "      <td>0.411770</td>\n",
              "      <td>2.789514</td>\n",
              "      <td>5.685038</td>\n",
              "      <td>0.211636</td>\n",
              "      <td>0.172817</td>\n",
              "      <td>2.292150</td>\n",
              "      <td>0.226972</td>\n",
              "      <td>1.596377</td>\n",
              "      <td>0.695006</td>\n",
              "      <td>0.299051</td>\n",
              "      <td>0.385987</td>\n",
              "      <td>0.281319</td>\n",
              "      <td>0.956676</td>\n",
              "      <td>0.587559</td>\n",
              "      <td>1.725774</td>\n",
              "      <td>2.043037</td>\n",
              "      <td>0.445222</td>\n",
              "      <td>0.834659</td>\n",
              "      <td>0.400364</td>\n",
              "      <td>0.356178</td>\n",
              "      <td>0.173680</td>\n",
              "      <td>1.761047</td>\n",
              "      <td>3.485287</td>\n",
              "      <td>1.509249</td>\n",
              "      <td>0.255727</td>\n",
              "      <td>0.304419</td>\n",
              "      <td>0.780504</td>\n",
              "      <td>0.157194</td>\n",
              "      <td>0.430940</td>\n",
              "      <td>2.921882</td>\n",
              "      <td>0.342279</td>\n",
              "      <td>0.423560</td>\n",
              "      <td>0.324835</td>\n",
              "      <td>0.761718</td>\n",
              "      <td>0.545097</td>\n",
              "      <td>0.420876</td>\n",
              "      <td>0.545097</td>\n",
              "      <td>0.368255</td>\n",
              "      <td>...</td>\n",
              "      <td>0.454519</td>\n",
              "      <td>0.276431</td>\n",
              "      <td>0.182086</td>\n",
              "      <td>0.847615</td>\n",
              "      <td>0.194815</td>\n",
              "      <td>1.439460</td>\n",
              "      <td>0.294060</td>\n",
              "      <td>0.354548</td>\n",
              "      <td>1.306323</td>\n",
              "      <td>0.171427</td>\n",
              "      <td>0.158129</td>\n",
              "      <td>0.184570</td>\n",
              "      <td>0.106592</td>\n",
              "      <td>0.150471</td>\n",
              "      <td>0.178309</td>\n",
              "      <td>0.134275</td>\n",
              "      <td>0.118235</td>\n",
              "      <td>0.238073</td>\n",
              "      <td>0.142037</td>\n",
              "      <td>0.457156</td>\n",
              "      <td>0.257632</td>\n",
              "      <td>1.671738</td>\n",
              "      <td>2.004605</td>\n",
              "      <td>0.109749</td>\n",
              "      <td>1.009883</td>\n",
              "      <td>0.849270</td>\n",
              "      <td>0.200404</td>\n",
              "      <td>0.116682</td>\n",
              "      <td>0.134762</td>\n",
              "      <td>0.106592</td>\n",
              "      <td>0.104315</td>\n",
              "      <td>0.441581</td>\n",
              "      <td>0.111974</td>\n",
              "      <td>0.135103</td>\n",
              "      <td>0.131119</td>\n",
              "      <td>1.743610</td>\n",
              "      <td>Control</td>\n",
              "      <td>Memantine</td>\n",
              "      <td>C/S</td>\n",
              "      <td>c-CS-m</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309_3</th>\n",
              "      <td>0.509183</td>\n",
              "      <td>0.730247</td>\n",
              "      <td>0.418309</td>\n",
              "      <td>2.687201</td>\n",
              "      <td>5.622059</td>\n",
              "      <td>0.209011</td>\n",
              "      <td>0.175722</td>\n",
              "      <td>2.283337</td>\n",
              "      <td>0.230247</td>\n",
              "      <td>1.561316</td>\n",
              "      <td>0.677348</td>\n",
              "      <td>0.291276</td>\n",
              "      <td>0.381002</td>\n",
              "      <td>0.281710</td>\n",
              "      <td>1.003635</td>\n",
              "      <td>0.602449</td>\n",
              "      <td>1.731873</td>\n",
              "      <td>2.017984</td>\n",
              "      <td>0.467668</td>\n",
              "      <td>0.814329</td>\n",
              "      <td>0.399847</td>\n",
              "      <td>0.368089</td>\n",
              "      <td>0.173905</td>\n",
              "      <td>1.765544</td>\n",
              "      <td>3.571456</td>\n",
              "      <td>1.501244</td>\n",
              "      <td>0.259614</td>\n",
              "      <td>0.311747</td>\n",
              "      <td>0.785154</td>\n",
              "      <td>0.160895</td>\n",
              "      <td>0.423187</td>\n",
              "      <td>2.944136</td>\n",
              "      <td>0.343696</td>\n",
              "      <td>0.425005</td>\n",
              "      <td>0.324852</td>\n",
              "      <td>0.757031</td>\n",
              "      <td>0.543620</td>\n",
              "      <td>0.404630</td>\n",
              "      <td>0.552994</td>\n",
              "      <td>0.363880</td>\n",
              "      <td>...</td>\n",
              "      <td>0.447197</td>\n",
              "      <td>0.256648</td>\n",
              "      <td>0.184388</td>\n",
              "      <td>0.856166</td>\n",
              "      <td>0.200737</td>\n",
              "      <td>1.524364</td>\n",
              "      <td>0.301881</td>\n",
              "      <td>0.386087</td>\n",
              "      <td>1.279600</td>\n",
              "      <td>0.185456</td>\n",
              "      <td>0.148696</td>\n",
              "      <td>0.190532</td>\n",
              "      <td>0.108303</td>\n",
              "      <td>0.145330</td>\n",
              "      <td>0.176213</td>\n",
              "      <td>0.132560</td>\n",
              "      <td>0.117760</td>\n",
              "      <td>0.244817</td>\n",
              "      <td>0.142445</td>\n",
              "      <td>0.510472</td>\n",
              "      <td>0.255343</td>\n",
              "      <td>1.663550</td>\n",
              "      <td>2.016831</td>\n",
              "      <td>0.108196</td>\n",
              "      <td>0.996848</td>\n",
              "      <td>0.846709</td>\n",
              "      <td>0.193685</td>\n",
              "      <td>0.118508</td>\n",
              "      <td>0.134762</td>\n",
              "      <td>0.108303</td>\n",
              "      <td>0.106219</td>\n",
              "      <td>0.435777</td>\n",
              "      <td>0.111883</td>\n",
              "      <td>0.133362</td>\n",
              "      <td>0.127431</td>\n",
              "      <td>1.926427</td>\n",
              "      <td>Control</td>\n",
              "      <td>Memantine</td>\n",
              "      <td>C/S</td>\n",
              "      <td>c-CS-m</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309_4</th>\n",
              "      <td>0.442107</td>\n",
              "      <td>0.617076</td>\n",
              "      <td>0.358626</td>\n",
              "      <td>2.466947</td>\n",
              "      <td>4.979503</td>\n",
              "      <td>0.222886</td>\n",
              "      <td>0.176463</td>\n",
              "      <td>2.152301</td>\n",
              "      <td>0.207004</td>\n",
              "      <td>1.595086</td>\n",
              "      <td>0.583277</td>\n",
              "      <td>0.296729</td>\n",
              "      <td>0.377087</td>\n",
              "      <td>0.313832</td>\n",
              "      <td>0.875390</td>\n",
              "      <td>0.520293</td>\n",
              "      <td>1.566852</td>\n",
              "      <td>2.132754</td>\n",
              "      <td>0.477671</td>\n",
              "      <td>0.727705</td>\n",
              "      <td>0.385639</td>\n",
              "      <td>0.362970</td>\n",
              "      <td>0.179449</td>\n",
              "      <td>1.286277</td>\n",
              "      <td>2.970137</td>\n",
              "      <td>1.419710</td>\n",
              "      <td>0.259536</td>\n",
              "      <td>0.279218</td>\n",
              "      <td>0.734492</td>\n",
              "      <td>0.162210</td>\n",
              "      <td>0.410615</td>\n",
              "      <td>2.500204</td>\n",
              "      <td>0.344509</td>\n",
              "      <td>0.429211</td>\n",
              "      <td>0.330121</td>\n",
              "      <td>0.746980</td>\n",
              "      <td>0.546763</td>\n",
              "      <td>0.386860</td>\n",
              "      <td>0.547849</td>\n",
              "      <td>0.366771</td>\n",
              "      <td>...</td>\n",
              "      <td>0.442650</td>\n",
              "      <td>0.398534</td>\n",
              "      <td>0.161768</td>\n",
              "      <td>0.760234</td>\n",
              "      <td>0.184169</td>\n",
              "      <td>1.612382</td>\n",
              "      <td>0.296382</td>\n",
              "      <td>0.290680</td>\n",
              "      <td>1.198765</td>\n",
              "      <td>0.159799</td>\n",
              "      <td>0.166112</td>\n",
              "      <td>0.185323</td>\n",
              "      <td>0.103184</td>\n",
              "      <td>0.140656</td>\n",
              "      <td>0.163804</td>\n",
              "      <td>0.123210</td>\n",
              "      <td>0.117439</td>\n",
              "      <td>0.234947</td>\n",
              "      <td>0.145068</td>\n",
              "      <td>0.430996</td>\n",
              "      <td>0.251103</td>\n",
              "      <td>1.484624</td>\n",
              "      <td>1.957233</td>\n",
              "      <td>0.119883</td>\n",
              "      <td>0.990225</td>\n",
              "      <td>0.833277</td>\n",
              "      <td>0.192112</td>\n",
              "      <td>0.132781</td>\n",
              "      <td>0.134762</td>\n",
              "      <td>0.103184</td>\n",
              "      <td>0.111262</td>\n",
              "      <td>0.391691</td>\n",
              "      <td>0.130405</td>\n",
              "      <td>0.147444</td>\n",
              "      <td>0.146901</td>\n",
              "      <td>1.700563</td>\n",
              "      <td>Control</td>\n",
              "      <td>Memantine</td>\n",
              "      <td>C/S</td>\n",
              "      <td>c-CS-m</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309_5</th>\n",
              "      <td>0.434940</td>\n",
              "      <td>0.617430</td>\n",
              "      <td>0.358802</td>\n",
              "      <td>2.365785</td>\n",
              "      <td>4.718679</td>\n",
              "      <td>0.213106</td>\n",
              "      <td>0.173627</td>\n",
              "      <td>2.134014</td>\n",
              "      <td>0.192158</td>\n",
              "      <td>1.504230</td>\n",
              "      <td>0.550960</td>\n",
              "      <td>0.286961</td>\n",
              "      <td>0.363502</td>\n",
              "      <td>0.277964</td>\n",
              "      <td>0.864912</td>\n",
              "      <td>0.507990</td>\n",
              "      <td>1.480059</td>\n",
              "      <td>2.013697</td>\n",
              "      <td>0.483416</td>\n",
              "      <td>0.687794</td>\n",
              "      <td>0.367531</td>\n",
              "      <td>0.355311</td>\n",
              "      <td>0.174836</td>\n",
              "      <td>1.324695</td>\n",
              "      <td>2.896334</td>\n",
              "      <td>1.359876</td>\n",
              "      <td>0.250705</td>\n",
              "      <td>0.273667</td>\n",
              "      <td>0.702699</td>\n",
              "      <td>0.154827</td>\n",
              "      <td>0.398550</td>\n",
              "      <td>2.456560</td>\n",
              "      <td>0.329126</td>\n",
              "      <td>0.408755</td>\n",
              "      <td>0.313415</td>\n",
              "      <td>0.691956</td>\n",
              "      <td>0.536860</td>\n",
              "      <td>0.360816</td>\n",
              "      <td>0.512824</td>\n",
              "      <td>0.351551</td>\n",
              "      <td>...</td>\n",
              "      <td>0.419095</td>\n",
              "      <td>0.393447</td>\n",
              "      <td>0.160200</td>\n",
              "      <td>0.768113</td>\n",
              "      <td>0.185718</td>\n",
              "      <td>1.645807</td>\n",
              "      <td>0.296829</td>\n",
              "      <td>0.309345</td>\n",
              "      <td>1.206995</td>\n",
              "      <td>0.164650</td>\n",
              "      <td>0.160687</td>\n",
              "      <td>0.188221</td>\n",
              "      <td>0.104784</td>\n",
              "      <td>0.141983</td>\n",
              "      <td>0.167710</td>\n",
              "      <td>0.136838</td>\n",
              "      <td>0.116048</td>\n",
              "      <td>0.255528</td>\n",
              "      <td>0.140871</td>\n",
              "      <td>0.481227</td>\n",
              "      <td>0.251773</td>\n",
              "      <td>1.534835</td>\n",
              "      <td>2.009109</td>\n",
              "      <td>0.119524</td>\n",
              "      <td>0.997775</td>\n",
              "      <td>0.878668</td>\n",
              "      <td>0.205604</td>\n",
              "      <td>0.129954</td>\n",
              "      <td>0.134762</td>\n",
              "      <td>0.104784</td>\n",
              "      <td>0.110694</td>\n",
              "      <td>0.434154</td>\n",
              "      <td>0.118481</td>\n",
              "      <td>0.140314</td>\n",
              "      <td>0.148380</td>\n",
              "      <td>1.839730</td>\n",
              "      <td>Control</td>\n",
              "      <td>Memantine</td>\n",
              "      <td>C/S</td>\n",
              "      <td>c-CS-m</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309_6</th>\n",
              "      <td>0.447506</td>\n",
              "      <td>0.628176</td>\n",
              "      <td>0.367388</td>\n",
              "      <td>2.385939</td>\n",
              "      <td>4.807635</td>\n",
              "      <td>0.218578</td>\n",
              "      <td>0.176233</td>\n",
              "      <td>2.141282</td>\n",
              "      <td>0.195188</td>\n",
              "      <td>1.442398</td>\n",
              "      <td>0.566340</td>\n",
              "      <td>0.289824</td>\n",
              "      <td>0.363893</td>\n",
              "      <td>0.266837</td>\n",
              "      <td>0.859121</td>\n",
              "      <td>0.521307</td>\n",
              "      <td>1.538244</td>\n",
              "      <td>1.968275</td>\n",
              "      <td>0.495900</td>\n",
              "      <td>0.672402</td>\n",
              "      <td>0.369404</td>\n",
              "      <td>0.357172</td>\n",
              "      <td>0.179728</td>\n",
              "      <td>1.227450</td>\n",
              "      <td>2.956983</td>\n",
              "      <td>1.447910</td>\n",
              "      <td>0.250840</td>\n",
              "      <td>0.284044</td>\n",
              "      <td>0.704396</td>\n",
              "      <td>0.156876</td>\n",
              "      <td>0.391047</td>\n",
              "      <td>2.467133</td>\n",
              "      <td>0.327598</td>\n",
              "      <td>0.404490</td>\n",
              "      <td>0.296276</td>\n",
              "      <td>0.674419</td>\n",
              "      <td>0.539723</td>\n",
              "      <td>0.354214</td>\n",
              "      <td>0.514316</td>\n",
              "      <td>0.347224</td>\n",
              "      <td>...</td>\n",
              "      <td>0.412824</td>\n",
              "      <td>0.382578</td>\n",
              "      <td>0.162330</td>\n",
              "      <td>0.779695</td>\n",
              "      <td>0.186793</td>\n",
              "      <td>1.634615</td>\n",
              "      <td>0.288037</td>\n",
              "      <td>0.332367</td>\n",
              "      <td>1.123445</td>\n",
              "      <td>0.175693</td>\n",
              "      <td>0.150594</td>\n",
              "      <td>0.183824</td>\n",
              "      <td>0.106476</td>\n",
              "      <td>0.139564</td>\n",
              "      <td>0.174844</td>\n",
              "      <td>0.130515</td>\n",
              "      <td>0.115243</td>\n",
              "      <td>0.236850</td>\n",
              "      <td>0.136454</td>\n",
              "      <td>0.478577</td>\n",
              "      <td>0.244485</td>\n",
              "      <td>1.507777</td>\n",
              "      <td>2.003535</td>\n",
              "      <td>0.120687</td>\n",
              "      <td>0.920178</td>\n",
              "      <td>0.843679</td>\n",
              "      <td>0.190469</td>\n",
              "      <td>0.131575</td>\n",
              "      <td>0.134762</td>\n",
              "      <td>0.106476</td>\n",
              "      <td>0.109446</td>\n",
              "      <td>0.439833</td>\n",
              "      <td>0.116657</td>\n",
              "      <td>0.140766</td>\n",
              "      <td>0.142180</td>\n",
              "      <td>1.816389</td>\n",
              "      <td>Control</td>\n",
              "      <td>Memantine</td>\n",
              "      <td>C/S</td>\n",
              "      <td>c-CS-m</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6 rows × 81 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         DYRK1A_N   ITSN1_N    BDNF_N  ...  Treatment  Behavior   class\n",
              "MouseID                                ...                             \n",
              "309_1    0.503644  0.747193  0.430175  ...  Memantine       C/S  c-CS-m\n",
              "309_2    0.514617  0.689064  0.411770  ...  Memantine       C/S  c-CS-m\n",
              "309_3    0.509183  0.730247  0.418309  ...  Memantine       C/S  c-CS-m\n",
              "309_4    0.442107  0.617076  0.358626  ...  Memantine       C/S  c-CS-m\n",
              "309_5    0.434940  0.617430  0.358802  ...  Memantine       C/S  c-CS-m\n",
              "309_6    0.447506  0.628176  0.367388  ...  Memantine       C/S  c-CS-m\n",
              "\n",
              "[6 rows x 81 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MrmA59JVhlg",
        "outputId": "5629c2b7-01fc-412c-fcfb-dc17641aa8ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df1.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1080, 81)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22EbuYoaK5FM"
      },
      "source": [
        "## Binary Classification for Down's Syndrome\n",
        "\n",
        "We will first predict the binary class label in `df1['Genotype']` which indicates if the mouse has Down's syndrome or not.  Get the string values in `df1['Genotype'].values` and convert this to a numeric vector `y` with 0 or 1.  You may wish to use the `np.unique` command with the `return_inverse=True` option."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dExZR0UHK5FM",
        "outputId": "ee067958-6bce-4241-b1f4-6c18b4ec1b06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# TODO\n",
        "#   y = ...\n",
        "temp = df1['Genotype'].values \n",
        "print(temp)\n",
        "yraw, y = np.unique(temp, return_inverse=True)\n",
        "#y.replace(to_replace='Control', value=1)\n",
        "#y.replace(to_repace='Ts65Dn', value =1)\n",
        "print(yraw)\n",
        "print(y)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Control' 'Control' 'Control' ... 'Ts65Dn' 'Ts65Dn' 'Ts65Dn']\n",
            "['Control' 'Ts65Dn']\n",
            "[0 0 0 ... 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWy8XtCJK5FP"
      },
      "source": [
        "As predictors, get all but the last four columns of the dataframes.  Store the data matrix into `X` and the names of the columns in `xnames`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDV1V0sMK5FP",
        "outputId": "c944cf89-42b0-41c9-c8fa-ad7c06e3c0fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# TODO\n",
        "#   xnames = ...\n",
        "#   X = ...\n",
        "xnames = df1.columns[0:77]\n",
        "#X = df1.as_matrix(xnames)\n",
        "#x_dash = df1[xnames].to_numpy()\n",
        "X = np.array(df1[xnames])\n",
        "print(X.shape)\n",
        "#print(x_dash)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1080, 77)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4_grUhdK5FR"
      },
      "source": [
        "Split the data into training and test with 30% allocated for test.  You can use the train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYbbVZGGK5FS",
        "outputId": "e92d797c-b4a5-4223-8db2-6db7ea59eace",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# TODO:\n",
        "#   Xtr, Xts, ytr, yts = ...\n",
        "Xtr, Xts, ytr, yts = train_test_split(X,y, test_size=0.30)\n",
        "print(Xtr.shape)\n",
        "print(Xts.shape)\n",
        "print(ytr.shape)\n",
        "print(yts.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(756, 77)\n",
            "(324, 77)\n",
            "(756,)\n",
            "(324,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wutLMvKwK5FT"
      },
      "source": [
        "Scale the data with the `StandardScaler`.  Store the scaled values in `Xtr1` and `Xts1`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOka6KT5K5FU"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# TODO\n",
        "#  Xtr1 = ...\n",
        "#  Xts1 = ...\n",
        "scal = StandardScaler()\n",
        "Xtr1 = scal.fit_transform(Xtr)\n",
        "Xts1 = scal.transform(Xts)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhksluBsK5FV"
      },
      "source": [
        "Create a `LogisticRegression` object `logreg` and `fit` on the scaled training data.  Set the regularization level to `C=1e5` and use the optimizer `solver=liblinear`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HI3dvai-K5FV",
        "outputId": "e005f1b2-43bb-4792-de1a-2ebb3c7c6e28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# TODO\n",
        "#   logreg = ...\n",
        "logreg = linear_model.LogisticRegression(C=1e5, solver='liblinear')\n",
        "logreg.fit(Xtr1, ytr)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
              "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
              "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyA3XvBTK5FX"
      },
      "source": [
        "Measure the accuracy of the classifer on test data.  You should get around 94%.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AW2sKoUZK5FX",
        "outputId": "a98b9633-c809-444b-b87d-7796a2f4ed1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# TODO\n",
        "#   yhat = ...\n",
        "yhat = logreg.predict(Xts1)\n",
        "acc = np.mean(yhat == yts)\n",
        "print(\"Accuracy on test data = %f\" % acc)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on test data = 0.953704\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHePM3DiK5FZ"
      },
      "source": [
        "## Interpreting the weight vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdYDWH5iK5FZ"
      },
      "source": [
        "Create a stem plot of the coefficients, `W` in the logistic regression model.  Jse the `plt.stem()` function with the `use_line_collection=True` option.  You can get the coefficients from `logreg.coef_`, but you will need to reshape this to a 1D array.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ek5VzX8vK5Fa",
        "outputId": "11a87544-1791-4053-ef9e-641df5005343",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        }
      },
      "source": [
        "# TODO\n",
        "#   W = ...\n",
        "#   plt.stem(...)\n",
        "W=logreg.coef_\n",
        "data = {'feature': xnames, 'slope': np.squeeze(W)}\n",
        "dfslope = pd.DataFrame(data=data)\n",
        "W_temp = dfslope.to_numpy()\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.stem(W_temp[:,1], use_line_collection=True)\n",
        "plt.xlabel('Variable index')\n",
        "plt.ylabel('W')\n",
        "plt.grid()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hi-BJSY-K5Fb"
      },
      "source": [
        "You should see that `W[i]` is very large for a few components `i`.  These are the genes that are likely to be most involved in Down's Syndrome.   Below we will use L1 regression to enforce sparsity.  Find the names of the genes for two components `i` where the magnitude of `W[i]` is largest.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GT8Uq_xSK5Fb",
        "outputId": "40183650-ed43-4ec1-ffec-e3820253ad0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# TODO\n",
        "W_new = sorted(W_temp, key=lambda x: x[1], reverse=True)\n",
        "print('Two components with highest magnitude = \\n{}, \\n{}'.format(W_new[0], W_new[1]))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Two components with highest magnitude = \n",
            "['ITSN1_N' 45.20861442488912], \n",
            "['TIAM1_N' 30.061024745865367]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Il4NqZQK5Fd"
      },
      "source": [
        "## Cross Validation\n",
        "\n",
        "To obtain a slightly more accurate result, now perform 10-fold cross validation and measure the average precision, recall and f1-score.  Note, that in performing the cross-validation, you will want to randomly permute the test and training sets using the `shuffle` option.  In this data set, all the samples from each class are bunched together, so shuffling is essential.  Print the mean precision, recall and f1-score and error rate across all the folds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjxwYYMPK5Fd",
        "outputId": "4679a5ea-6d51-4b3c-88d5-c06dfca9fca1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "nfold = 10\n",
        "kf = KFold(n_splits=nfold,shuffle=True)\n",
        "\n",
        "# TODO\n",
        "acc = np.zeros(nfold)\n",
        "prec = np.zeros(nfold)\n",
        "rec = np.zeros(nfold)\n",
        "f1 = np.zeros(nfold)\n",
        "\n",
        "for i, I in enumerate(kf.split(X)):\n",
        "    \n",
        "    # Get training and test data\n",
        "    train, test = I\n",
        "    Xtr = X[train,:]\n",
        "    ytr = y[train]\n",
        "    Xts = X[test,:]\n",
        "    yts = y[test]\n",
        "    \n",
        "    # Scale the data\n",
        "    scal = StandardScaler()\n",
        "    Xtr1 = scal.fit_transform(Xtr)\n",
        "    Xts1 = scal.transform(Xts)    \n",
        "    \n",
        "    # Fit a model    \n",
        "    logreg.fit(Xtr1, ytr)\n",
        "    \n",
        "    # Predict on test samples and measure accuracy\n",
        "    yhat = logreg.predict(Xts1)\n",
        "    acc[i] = np.mean(yhat == yts)\n",
        "    \n",
        "    # Measure other performance metrics\n",
        "    prec[i],rec[i],f1[i],_  = precision_recall_fscore_support(yts,yhat,average='binary') \n",
        "    \n",
        "\n",
        "# Take average values of the metrics\n",
        "precm = np.mean(prec)\n",
        "recm = np.mean(rec)\n",
        "f1m = np.mean(f1)\n",
        "accm= np.mean(acc)\n",
        "\n",
        "# Compute the standard errors\n",
        "prec_se = np.std(prec)/np.sqrt(nfold-1)\n",
        "rec_se = np.std(rec)/np.sqrt(nfold-1)\n",
        "f1_se = np.std(f1)/np.sqrt(nfold-1)\n",
        "acc_se = np.std(acc)/np.sqrt(nfold-1)\n",
        "\n",
        "print('Precision = {0:.4f}, SE={1:.4f}'.format(precm,prec_se))\n",
        "print('Recall =    {0:.4f}, SE={1:.4f}'.format(recm, rec_se))\n",
        "print('f1 =        {0:.4f}, SE={1:.4f}'.format(f1m, f1_se))\n",
        "print('Accuracy =  {0:.4f}, SE={1:.4f}'.format(accm, acc_se))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision = 0.9457, SE=0.0094\n",
            "Recall =    0.9675, SE=0.0081\n",
            "f1 =        0.9560, SE=0.0058\n",
            "Accuracy =  0.9583, SE=0.0059\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mx0RcJhRK5Fe"
      },
      "source": [
        "## Multi-Class Classification\n",
        "\n",
        "Now use the response variable in `df1['class']`.  This has 8 possible classes.  Use the `np.unique` funtion as before to convert this to a vector `y` with values 0 to 7."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-bIeJTTK5Ff",
        "outputId": "f56a9493-b775-42e1-80f9-a4bd37161043",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# TODO\n",
        "#   y = ...\n",
        "yraw1, y1 = np.unique(df1['class'], return_inverse=True)\n",
        "print(y1)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 ... 7 7 7]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsD4eAloK5Fg"
      },
      "source": [
        "Fit a multi-class logistic model by creating a `LogisticRegression` object, `logreg` and then calling the `logreg.fit` method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7Bas5OwK5Fg"
      },
      "source": [
        "Now perform 10-fold cross validation, and measure the confusion matrix `C` on the test data in each fold. You can use the `confustion_matrix` method in the `sklearn` package.  Add the confusion matrix counts across all folds and then normalize the rows of the confusion matrix so that they sum to one.  Thus, each element `C[i,j]` will represent the fraction of samples where `yhat==j` given `ytrue==i`.  Print the confusion matrix.  You can use the command\n",
        "\n",
        "    print(np.array_str(C, precision=4, suppress_small=True))\n",
        "    \n",
        "to create a nicely formatted print.  Also print the overall mean and SE of the test accuracy across the folds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBfoacitK5Fh",
        "outputId": "5d4b1d6a-7db3-4cbb-ff4a-1d3db225db67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# TODO\n",
        "nfold = 10\n",
        "kf = KFold(n_splits=nfold,shuffle=True)\n",
        "\n",
        "C = np.zeros([8,8])\n",
        "for i, I in enumerate(kf.split(X)):\n",
        "    \n",
        "    # Get training and test data\n",
        "    train, test = I\n",
        "    Xtr = X[train,:]\n",
        "    ytr = y1[train]\n",
        "    Xts = X[test,:]\n",
        "    yts = y1[test]\n",
        "    # Scale the data\n",
        "    scal = StandardScaler()\n",
        "    Xtr1 = scal.fit_transform(Xtr)\n",
        "    Xts1 = scal.transform(Xts)    \n",
        "    \n",
        "    # Fit a model    \n",
        "    logreg.fit(Xtr1, ytr)\n",
        "    \n",
        "    # Predict on test samples and measure accuracy\n",
        "    yhat = logreg.predict(Xts1)\n",
        "    acc[i] = np.mean(yhat == yts)\n",
        "\n",
        "    C+= confusion_matrix(yts, yhat, labels=np.unique(y1))\n",
        "\n",
        "print(np.array_str(C, precision=4, suppress_small=True))\n",
        "\n",
        "for i in range(8):\n",
        "  C[i]=C[i]/np.sum(C[i])\n",
        "print(np.array_str(C, precision=4, suppress_small=True))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[146.   3.   0.   0.   0.   1.   0.   0.]\n",
            " [  3. 130.   0.   0.   1.   0.   1.   0.]\n",
            " [  0.   1. 148.   0.   1.   0.   0.   0.]\n",
            " [  1.   0.   0. 134.   0.   0.   0.   0.]\n",
            " [  0.   1.   0.   0. 134.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0. 105.   0.   0.]\n",
            " [  0.   0.   0.   1.   0.   0. 134.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   0. 135.]]\n",
            "[[0.9733 0.02   0.     0.     0.     0.0067 0.     0.    ]\n",
            " [0.0222 0.963  0.     0.     0.0074 0.     0.0074 0.    ]\n",
            " [0.     0.0067 0.9867 0.     0.0067 0.     0.     0.    ]\n",
            " [0.0074 0.     0.     0.9926 0.     0.     0.     0.    ]\n",
            " [0.     0.0074 0.     0.     0.9926 0.     0.     0.    ]\n",
            " [0.     0.     0.     0.     0.     1.     0.     0.    ]\n",
            " [0.     0.     0.     0.0074 0.     0.     0.9926 0.    ]\n",
            " [0.     0.     0.     0.     0.     0.     0.     1.    ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNlj_ED6K5Fi"
      },
      "source": [
        "Re-run the logistic regression on the entire training data and get the weight coefficients.  This should be a 8 x 77 matrix.  Create a stem plot of the first row of this matrix to see the coefficients on each of the genes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaWGycnSK5Fi",
        "outputId": "31beefc9-02b4-469e-9e61-a09e6a38cb3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        }
      },
      "source": [
        "# TODO\n",
        "logreg = linear_model.LogisticRegression(C=1e5, solver='liblinear')\n",
        "logreg.fit(Xtr, ytr)\n",
        "yhat = logreg.predict(Xtr)\n",
        "acc = np.mean(yhat == yts)\n",
        "W3=logreg.coef_\n",
        "print(W3.shape)\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.stem(W3[0,:], use_line_collection=True)\n",
        "plt.xlabel('Variable index')\n",
        "plt.ylabel('W')\n",
        "plt.grid()\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(8, 77)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmwAAAHgCAYAAAAYDzEbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dfZRcdZ3n8c+XJmgJaos4LV2giS42i0ZpadEY1+mgM40PO7QZdXBnXVD3xD2r4zirrWndM+J4mGQ2M+voqpyNysrsrgmIoWHBpVViHxyGp2ArAbTXCPJQ4WEUCg2WJnS++0fdSnU31dVd1VX3/u6979c5Oem6VdX161/de+tTv6dr7i4AAACE66ikCwAAAIDmCGwAAACBI7ABAAAEjsAGAAAQOAIbAABA4AhsAAAAgTs66QJ00wknnOCrV6/u+us88cQTOvbYY7v+OmlAXVRRD3XURR11UUddVFEPddSFdNttt/3C3Z/X6L5MB7bVq1drz549XX+dqakpDQ8Pd/110oC6qKIe6qiLOuqijrqooh7qqAvJzO5d7D66RAEAAAJHYAMAAAgcgQ0AACBwBDYAAIDAEdgAAAACR2ADAAAIHIENAAAgcAQ2AACAwBHYAAAAAkdgAwAACByBDQAAIHAENgAAgMAR2AAAAAJHYAMAAAgcgQ0AACBwiQY2M7vYzB4xszvmbLvAzEpm9sPo35vn3DduZvvMbMbMRpIpNQAgzSamS1q/dbfWbL5G67fu1sR0KekiAUs6OuHX/5qkL0j6hwXbP+vufzt3g5mdJulcSS+V1C/pu2b2EnefjaOgAID0m5guaXzXXlUOVT86SuWKxnftlSSNDhaTLBrQVKItbO5+vaRHl/nwcyTtdPffufs9kvZJOrNrhQMAZM62yZkjYa2mcmhW2yZnEioRsDzm7skWwGy1pKvd/WXR7QsknS/pV5L2SPqIuz9mZl+QdJO7/6/ocV+V9H/d/fIFv2+TpE2S1NfXd8bOnTu7/jccOHBAxx13XNdfJw2oiyrqoY66qKMu6pKqi/OvfWLR+7529rExlqSKfaKOupA2bNhwm7sPNbov6S7RRi6S9BlJHv3/d5Leu9wnu/t2SdslaWhoyIeHh7tQxPmmpqYUx+ukAXVRRT3UURd11EVdUnVRvGm3SuXKU7f3FhIpD/tEHXXRXHCzRN39YXefdffDkr6serdnSdLJcx56UrQNAIBlGRsZUGFVz7xthVU9GhsZSKhEwPIEF9jM7MQ5N98mqTaD9CpJ55rZ08xsjaRTJN0Sd/kAAOk1OljUlo1rdUxP9eOv2FvQlo1rmXCA4CXaJWpmOyQNSzrBzB6Q9ClJw2Z2uqpdoj+X9H5Jcvc7zewySXdJelLSB5ghCgBo1ehgUTtuuU+SdOn71yVcGmB5Eg1s7v6uBpu/2uTxF0q6sHslAgAACE9wXaIAAACYj8AGAAAQOAIbAABA4AhsAAAAgSOwAQAABI7ABgAAEDgCGwAAQOAIbAAAAIEjsAEAAASOwAYAABA4AhsAAEDgCGwAAACBI7ABAAAEjsAGAAAQOAIbAABA4AhsAAAAgSOwAQAABI7ABgAAEDgCGwAAQOAIbAAAAIEjsAEAAASOwAYAABA4AhsAAEDgCGwAAACBI7ABAAAEjsAGAAAQOAIbAABA4AhsAAAAgSOwAQAABI7ABgAAEDgCGwAAQOAIbAAAAIEjsAEAAASOwAYAABA4AhsAAEDgCGwAAACBI7ABAAAEjsAGAAAQOAIbAABA4AhsAAAAgSOwAQAABI7ABgAAELijky4Asm1iuqRtkzPaX66ov7egsZEBjQ4Wky4WAACpQmBD10xMlzS+a68qh2YlSaVyReO79koSoQ0AgBbQJYqu2TY5cySs1VQOzWrb5ExCJQIAIJ0IbOia/eVKS9sBAEBjBDZ0TX9voaXtAACgMQIbumZsZECFVT3zthVW9WhsZCChEgEAkE5MOkDX1CYWfOzy23Vw9rCKzBIFAKAtBDZ01ehgUTtuuU+SdOn71yVcGgAA0okuUQAAgMAR2AAAAAJHYAMAAAgcgQ0AACBwBDYAAIDAEdgAAAACR2ADAAAIHIENAAAgcAQ2AACAwBHYAAAAAkdgAwAACByBDQAAIHAENgAAgMAR2AAAAAJHYAMAAAgcgQ0AACBwBDYAAIDAEdgAAAACR2ADAAAIXKKBzcwuNrNHzOyOOduON7PvmNlPo/+fE203M/u8me0zs9vN7JXJlRwAACA+SbewfU3S2Qu2bZZ0nbufIum66LYkvUnSKdG/TZIuiqmMAAAAiUo0sLn79ZIeXbD5HEmXRD9fIml0zvZ/8KqbJPWa2YnxlBQAACA55u7JFsBstaSr3f1l0e2yu/dGP5ukx9y918yulrTV3f8xuu86SR939z0Lft8mVVvg1NfXd8bOnTu7/jccOHBAxx13XNdfJw0a1cWWmyuSpPFXF5IoUiLYJ+qoizrqoi7pugjlvJR0PYSEupA2bNhwm7sPNbrv6LgL0wp3dzNrKVG6+3ZJ2yVpaGjIh4eHu1G0eaamphTH66RBo7q4aOZGSdLw8LoESpQM9ok66qKOuqhLui5COS8lXQ8hoS6aS3oMWyMP17o6o/8fibaXJJ0853EnRdsAAAAyLcTAdpWk86Kfz5N05Zzt/y6aLfoaSY+7+4NJFBAAACBOiXaJmtkOScOSTjCzByR9StJWSZeZ2fsk3SvpndHDvyXpzZL2SfqNpPfEXmAAAIAEJBrY3P1di9z1hgaPdUkf6G6JAAAAwhNilygAAADmILABAAAEjsAGAAAQOAIbAABA4AhsAAAAgSOwAQAABI7ABgAAEDgCGwAAQOAIbAAAAIEjsAEAAASOwAYAABA4AhsAAEDgCGwAAACBI7ABAAAEjsAGAAAQuKOTLgAALMfEdEnbJme0v1xRf29BYyMDGh0sJl0sAIgFgQ1A8CamSxrftVeVQ7OSpFK5ovFdeyWJ0AYgF+gSBRC8bZMzR8JaTeXQrLZNziRUIgCIF4ENQPD2lystbQeArCGwAQhef2+hpe0AkDUENgDBGxsZUGFVz7xthVU9GhsZSKhEABAvJh0ACF5tYsHHLr9dB2cPq8gsUQA5Q2ADkAqjg0XtuOU+SdKl71+XcGkAIF50iQIAAASOwAYAABA4AhsAAEDgCGwAAACBI7ABAAAEjsAGAAAQOAIbAABA4AhsAAAAgSOwAQAABI7ABgAAEDgCGwAAQOC4ligAAJhnYrqkbZMz2l+uqL+3oLGRAY0OFpMuVq4R2AAAwBET0yWN79qryqFZSVKpXNH4rr2SRGhLEIENAIAVyFpr1LbJmSNhraZyaFbbJmdS/XelHYENAIA2ZbE1an+50tJ2xINJB0jMxHRJ67fu1prN12j91t2amC4lXSQAaEmz1qi06u8ttLQd8SCwIRG1b6WlckWu+rdSQhuANMlia9TYyIAKq3rmbSus6tHYyEBCJYJEYENCsvitFED+ZLE1anSwqC0b1+qYnmpEKPYWtGXj2tR28WYFgQ2JyOK3UgD5k9XWqNHBogZf0KtXrzleN2w+i7AWAAIbEpHFb6UA8ofWKMSFwIZEZPVbKYD8oTUKcWBZDySidkL72OW36+DsYRUzsHYRAADdQmBDYkYHi9pxy32SpEvfvy7h0gAAEC4CG4AlZW0ldwBIGwIbgKayuJI7AKQNkw4ANMWaeQCQPAIbgKZYMw8AkkdgA9AUa+YBQPIIbACaYs08AEgekw4ANMWaeQCQPAIbgCWxZh4AJIsuUQAAgMAR2AAAAAJHYAMAAAgcgQ0AACBwBDYAAIDAEdgAAAACR2ADAAAIHIENAAAgcAQ2AACAwBHYAAAAAkdgAwAACByBDQAAIHAENgAAgMAR2AAAAAJHYAMAAAjc0UkXYDFm9nNJv5Y0K+lJdx8ys+MlXSpptaSfS3qnuz+WVBkBAADiEHoL2wZ3P93dh6LbmyVd5+6nSLouug0AAJBpoQe2hc6RdEn08yWSRhMsCwAAQCxCDmwu6dtmdpuZbYq29bn7g9HPD0nqS6ZoAAAA8Ql2DJuk17l7ycx+T9J3zOwnc+90dzczX/ikKNxtkqS+vj5NTU11vaAHDhyI5XXSoFFdlMsVSWpYR83uS7Ms7hPtvledrIu07y9Z3C/alXRddHpfCuH46LS4j7eQ6yIEwQY2dy9F/z9iZldIOlPSw2Z2ors/aGYnSnqkwfO2S9ouSUNDQz48PNz1sk5NTSmO10mDRnVx0cyNkqTh4XVPeXyz+9Isi/tEu+9VJ+si7ftLFveLdiVdF53el0I4Pjot7uMt5LoIQZBdomZ2rJk9s/azpD+UdIekqySdFz3sPElXJlNCAACA+ITawtYn6Qozk6pl/Lq7X2tmt0q6zMzeJ+leSe9MsIwAAACxCDKwufvdkl7RYPsvJb0h/hIBAAAkJ8guUQAAANQR2AAAAAJHYAMAAAgcgQ0AACBwBDYAAIDABTlLFACALJuYLmnb5IxK5YqKN+3W2MiARgeLSRcLASOwAQAQo4npksZ37VXl0KwkqVSuaHzXXkkitGFRdIkCABCjbZMzR8JaTeXQrLZNziRUIqQBgQ0AgBjtjy6qvtztgERgAwAgVv29hZa2AxKBDR0yMV3S+q27df61T2j91t2amC4lXSQACNLYyIAKq3rmbSus6tHYyEBCJUIaMOkAK9aNAbS1GVT7yxX19xaYQQUgM2rnso9dfrsOzh5WkXMcloHAhhVrNoC2nRMQM6gAZN3oYFE7brlP5XJZkx8/a959fGFFIwQ2rFinB9B2OgC2i5MmgLil4Qsr58ZkMIYNK9bpAbQhzKCqnTRL5Ypc9ZMmY/MAdFPoS35wbkwOgQ0r1ukBtCHMoAr9pAkgm0L4wtoM58bkENiwYqODRW3ZuFbH9FR3p2JvQVs2rm27iTyEGVShnzQBZFMIX1ib4dyYHAIbOmJ0sKjBF/Rq4DlH6YbNZ61oPEOnA2A7Qj9pAsimEL6wNsO5MTkENgSpFgBfveb4FQfAdoR+0gSQTSF8YW2Gc2NymCUKNMA6SUA6ZHHGYm3JD0m69P3rEi7NfN04N9bew1K5ouJNuzPxHnYDgQ1YRMgnTQDpWAIjizp5buQ9XD66RAEAqcSMxfTjPVw+WtgAZFYWu8tQx4zF9OM9XD5a2ABkEgt8Zh8zFtOP93D5CGwAMomuluxjxmL68R4uH12iADKJrpbsYzZ3+vEeLh+BDanCmCQsV39vQaUG4YyulmxhNnf61d7DcrmsyY+flXRxgkWXKFKDMUloBV0tALKEwIbUYEwSWhH6ivEA0Aq6RJEajElCq+guA5AVtLAhNZj+DQDIKwJbhk1Ml7R+626t2XyN1m/dnfqxXoxJAgDkVdMuUTP7sKR/kvQDd38yniKhFYvNmszi9dmY/g0AyKulxrCdJOnvJZ1qZnsl3aBqgPsnd3+024VDc81CWbMB+mkOOIxJAgDkUdPA5u4flSQzO0bSkKTXSnqPpO1mVnb307pfRCymWShjgD4AANmx3DFsBUnPkvTs6N9+STd3q1BYnmahjAH6AABkR9PAZmbbzewGSZdKWqdqd+g73H3I3d8TRwGxuGahjAH6AABkx1ItbC+Q9DRJD0kqSXpAUrnbhcLyNAtlLBoKAEB2LDWG7WwzM0kvVXX82kckvczMHpV0o7t/KoYyYhFLzZpkgD4AANmw5JUO3N0l3WFmZUmPR//eKulMSQS2hBHKAADIvqXWYfuQqi1rr5V0SNGSHpIulrS366UDsKjF1uADAGTPUi1sqyV9Q9JfuPuD3S8OgOXI4sLIAIDFNZ104O7/yd2/SVgDwtJsDT4AQPYsOYYNQHhYGBlAiBiq0T0ENiCF+nsLKjUIZ0stjMzJdGWoP2BxDNXoruVe6QBAQNpZGLl2Mi2VK3LVT6YT06UulzYbqD+gOYZqdBeBDUihdhZG5mS6MtQf0BxDNbqLLtGY0aWCTml1DT5OpvO1eixSf0Bz7Q7VwPLQwhYjulSQpGbXns2bdo5F6g9ojmtYdxeBLUZ0qSBJnEzr2jkWqT+gOa5h3V10icaILhUkaalrz+ZJO8fiSuqv1v1aKldUvGl3busd2cflEruHwBYj+veRNE6mVe0ei+3UH0sdAOgEukRjRJdKdkxMl7R+626t2XyN1m/dzTjElInzWGQoxHy1Y+f8a5/g2AFaQAtbjOiSygZaTNIvzmORoRB1HDtA+whsMUtzlxRLklQ1azHJY32kVVzHIkMh6jh2gPYR2LAsfDOuo8UErRgbGZh37Ej5HQrBsROetH4RT2u5V4LAlhJJ75x8M66jxQStSMNQiLjOLxw7YUnrF/G0lnulmHSQAiEsuMs34zomj6BVo4NFDb6gVwPPOUo3bD4rqA+VOM8vHDthSeuEmLSWe6UIbCkQws7JKu91aVgcklmsWK44zy9pOHbyJK1fxNNa7pUisKVACDsn34znq7WYvHrN8bluMUH6xX1+Cbm1MW/S+kU8reVeKQJbCoSwc/LNOD1CaJFNuzy1UIZwfkEy0vpFPK3lXikCWwqEsnOG3KqEuhBaZNMsby2UoZxfEL+0fhFPa7lXilmiKZCGWWYIBzPxViZvM6I5v+RbWtcGTWu5V4LAlhJ53DnRHtb9Wpk8tlByfumepJdkQnYQ2ICMocVkZWihRKfkdb0wdAdj2IAMYrxh+xjThU5hAhA6iRY2AJiDFkp0Sh6719E9BDYAWIAxXegEutfRSXSJAgDQBXSvo5NS18JmZmdL+pykHklfcfetCRcJAICn6Eb3OrNO8ytVgc3MeiR9UdIfSHpA0q1mdpW735VsyQAAeKpOdq8z6zTf0tYleqakfe5+t7sflLRT0jkJlwkAgK5j1mm+mbsnXYZlM7O3Szrb3f99dPvdkl7t7h9s9PihoSHfs2dPV8v00F//tR6+8Sb19vYu+zl3PfgrSdJpJz6rpdda7HnNfl+7r9VqGWr3Pfnkk3r5ycd3pHzt/L2d1m7Zy+VyS/tEu+Ksv3af18m6aKcMoex/ix0foYj7uOpkXXR6v+h0GZrtS4vVQ6Pn3HT3Lxd9/de86LmtF3oZ5Wu0fSX3NStDq/tEnPusJD3tX56q53/iE119DTO7zd2HGt6XtcBmZpskbZKkvr6+M3bu3NnVMv3k819X/y9LeuGz5g8sve/XhyVJL3hma42Y7T6vHYu91krKMDs7q56enqUf2CXNyt6Nv3exMrj7U/aJUMrXTDuvtdTf1KguHj/oeuiJw3KXVh1lOuEZpmcfY22XYSXP66SlytDo+Ghnn2h2X7u/r5luvFYn66LVcrf7vG7sY62cM39WPqxDh5/6mb3qKNOLe4/qynvfSe3sE8t5Xquv1e7+XDq+X6d+6N8suwzt2LBhQ2YC2zpJF7j7SHR7XJLcfUujx8fRwvYn//1GlctlTX78TU/ZLrU+ZqHd57VjsddaSRmmpqY0PDy84rK1q1nZu/H3LlaGRvtEKOVrpp3XWupvWlgXC8fhSNWZc7WLN6fh2Gm3DI2Oj3b2iWb3tfv7munGa3WyLlotd7vP68Y+1so5cyXHTlqPj+U8r9XXivPYaVWzFra0jWG7VdIpZrbGzI6RdK6kqxIuE4AWMA4n+yamS5q+r6yb73lU67fu1sR0KekiZcLoYFFbNq5Vsbcgk1TsLRwJa8i+VM0SdfcnzeyDkiZVXdbjYne/M+FidUztJHdw9rDWb93NdG1kEqu/Z1utFejgbLV7iZmMnTU6WKQecyptLWxy92+5+0vc/cXufmHS5emUxU5yfDNF1iy2yjurv2dDKC2otPIha1IX2LIqlJMc0G2s/p5tIbSg5vELMAE1+whsgQjhJAe0qp0PCcbhZFsILah5+wKcx4CaRwS2Fah9WM08dnjF32hCOMkBrVjJh8ToYFE3bD5L92x9i27YfBZhLUPabUHtZAtR3r4A5y2g5hWBrU2d/kZDNxHShg+JfFgsSC22vZ0W1E6fT/P2BThvATWvUjVLNCTNPqzaaS2oPYeL+qZbp2f6hjxzmA+J7FssSO2591F987bSojNBW53J2Onz6djIQMP1yrL6Bbi/t6BSg+MuqwE1rwhsberGhxXTtdOt08sZhL48Ah8S2bdYkNpx8/2aXbDo+koCVqfPp3n7Apy3gJpXdIm2Ke1N7swo6rxOdxGG3uVIN372LRaYFoa1pR6/lG6cT/M0TpKJPPlAC1ub0vyNJvSWm3Yl3X3Y6VaC0Lsc89aKkUeLtaL2mDUMbe0GrDSfT0NBD032EdjaNPfDqlSuqJiiD6tOjxcJQQghtNNdhGnocuRDItsWC1J/fEZR37yt1LGARfgHlkZgW4Hah1XSFzxvVegtN+0IIYR2upWAVoflS7p1NauaBamhFx7f0YBF+AeaI7DlUBpabloVQgjtdKsrrQ7LE0LrapYtFqQIWEC8CGw5lMWWm1BCaKdbXflQXFoIrasA0G3MEs2hLM4oYsZifoXQugoA3UYLW05lreWG7sP8CqV1NQSM5QOyi8CGzMhaCMXyZLGLvx2M5QOyjS5RAKkWShd/0otRh77QMoCVoYUNQOol3boaQusWY/mAbKOFDYhJ0i0w6J4QWrfSfrk8AM0R2IAYLNYCUwtthLl0C6F1i5nSQLbRJQrEYKkWmKS707AyIcxUZaY0kG0ENuRaXMsgNGuBYeHX9AtlpmrSY/kAdA9dositpbopO6nZ+KIQutOwMqHMVAWQXbSwIbfibNlq1gJTu/boQgwWTxdatwB0E4ENuRVny9ZS44tC6E4DAISLwIbcinug+GItMAwWBwAshcCG3AploLhEdxqeiuuCApiLwIbcomULoQrhygkAwkJgQ67RsoUQsdQLgIVY1gMAAsNSLwAWIrABQGC4LiiAhQhsABAYrgsKYCHGsAFAYJgQA2AhAhsABIgJMQDmoksUAAAgcAQ2AACWUFvI+OZ7HtX6rbs1MV1KukjIGQIbAABNLLaQMaENcSKwAQDQRLOFjIG4ENgAAGiChYzzLZTucAIbAABNsJBxfoXUHU5gAwCgCRYyzq+QusNZhw0AgCZYyDi/QuoOJ7ABALAEFjLOp/7egkoNwlkS3eF0iQIAADQQUnc4LWwAAAANhNQdTmADAAC5UFui4+DsYa3funtZ4SuU7nC6RAEAQOaFtERHOwhsAAAg80JaoqMdBDYAAJB5IS3R0Q4CGwAAyLy0X7GCwAYAADIvpCU62sEsUQAAkHkhLdHRDgIbAADIhVCW6GgHXaIAAACBI7ABAAAEjsAGAAAQOAIbAABA4AhsAAAAgSOwAQAABI7ABgBAhk1MlzR9X1k33/Oo1m/dnZqLnWM+AhsAABk1MV3S+K69Ojh7WJJUKlc0vmsvoS2FCGwAAGTUtskZVQ7NzttWOTSrbZMzCZUI7SKwAUDK0MWF5dpfrrS0HeEisAFAitDFhVb09xZa2o5wEdgAIEXo4uqurLVejo0MqLCqZ962wqoejY0MJFQitIuLvwNAitDF1T2LtV5KSu0Fw2vl3jY5o/3livp7CxobGUjt35NnBDYASJH+3oJKDcIZXVwr16z1Ms0BZ3SwmOryo4ouUQBIEbq4uofWS4SMwAYAKTI6WNSWjWtV7C3IJBV7C9qycS0tKB3AAH2EjC5RAEgZuri6Y2xkQOO79s7rFqX1EqEIroXNzC4ws5KZ/TD69+Y5942b2T4zmzGzkSTLCQDIFlovEbJQW9g+6+5/O3eDmZ0m6VxJL5XUL+m7ZvYSd59t9AsAAGgVrZcIVXAtbE2cI2mnu//O3e+RtE/SmQmXCQAABCRra+nVhBrYPmhmt5vZxWb2nGhbUdL9cx7zQLQNAAAg01cCSaRL1My+K+n5De76pKSLJH1Gkkf//52k97bwuzdJ2iRJfX19mpqaWmlxl3TgwIGnvE45mgYex+t32krK3qguQhHnexJyPTTT6ToqlyuanZ1t6fel+dhZStLnipDqttVjJKSyd1JazxXtWOo9XKwuWnnvPzP1G1UO+bxtlUOz+syVP1Lv4z9dUfmSlkhgc/c3LudxZvZlSVdHN0uSTp5z90nRtoW/e7uk7ZI0NDTkw8PDKyrrckxNTWnh61w0c6MkaXh4Xddfv9NWUvZGdRGKON+TkOuhmU7X0UUzN6pcLrdUF2k+dpaS9LkipLpt9RgJqeydlNZzRTuWeg8Xq4tW3vtHr72m8fbf+pL1HPo+FlyXqJmdOOfm2yTdEf18laRzzexpZrZG0imSbom7fACWpzaOZOaxw5kaRwIgXFleSy+4wCbpv5jZXjO7XdIGSX8hSe5+p6TLJN0l6VpJH2CGKBCmLI8jARCuLF8JJLhlPdz93U3uu1DShTEWB0AbsnpNRgBhy/LF7oMLbADSj2syAkhKVtfSC7FLFEDKZXkcCQAkgcAGoOOyPI4EAJJAlyiAjps7jqRUrqiYoXEkAJAEAhvmqS3FcHC2uhQDH7JoV20cSZ7WmQKQTmn47KNLFEewFAMAIG/S8tlHYMMRzZZiAJBuWb0gNrBSafnsI7DhCJZiALIpLS0IQBLS8tlHYMMRLMUAZFNaWhCAJKTls4/AhiNYigHIprS0IABJSMtnH7NEcUSWL+kB5Fl/b0GlBuEstBYEIAlp+ewjsGGerF7SA8izsZEBje/aO69bNMQWBCApafjso0sUADJudLCoLRvXqthbkEkq9ha0ZePa4D+gAGY319HCBgA5kIYWBGCuxWY3S8rlvkwLGwAACA6zm+cjsAEAgOAwu3k+AhsAAAhOWtZHiwuBDQAABCct66PFhUkHAAAgOGlZHy0uBDYAABAkZjfX0SWKXGAtHwBAmhHYuoBwEJbF1vLhfQEApAWBrcMIB+FhLR8AQNoR2DqMcBAe1vIBgGTR87RyBLYOIxyEh7V8ACA59Dx1BoGtwwgH4WEtH4SKVgfkAT1PnUFg6zDCQXhGB4vasnGtir0FmaRib0FbNq5lqjgSRasD8oKep85gHbYOY6G/MLGWz9JqrT0HZw9r/dbd7Ldd1qzVgXpHlvT3FlRqEM7oeWoNga0LCAdIm8VaeySxL3cJrQ7Ii7GRAY3v2jvvCwo9T62jSxQAY0wSwHhX5AXDUjqDFjYAtPYkgFYH5Ak9T94ZVDsAAAxASURBVCtHCxsAWnsSQKsDgFbQwgaA1p6E0OoAYLkIbACY3QwAgSOwAZBEaw8AhIwxbAAAAIEjsAEAAASOwAYAABA4AhsAAEDgCGwAAACBI7ABAAAEjsAGAAAQOAIbAABA4AhsAAAAgSOwAQAABI7ABgAAEDgCGwAAQOAIbAAAAIEjsAEAAASOwAYAABA4AhsAAEDgCGwAAACBI7ABAAAEjsAGAAAQOAIbACBoE9MlTd9X1s33PKr1W3drYrqUdJGA2BHYAADBmpguaXzXXh2cPSxJKpUrGt+1l9CG3CGwAQCCtW1yRpVDs/O2VQ7NatvkTEIlApJBYAMABGt/udLSdiCrCGwAgGD19xZa2g5kFYENABCssZEBFVb1zNtWWNWjsZGBhEoEJOPopAsAAMBiRgeLkqpj2faXK+rvLWhsZODIdiAvCGwAgKCNDhYJaMg9ukQBAAACR2ADAAAIHIENAAAgcAQ2AACAwBHYAAAAAkdgAwAACByBDQAAIHCJBDYze4eZ3Wlmh81saMF942a2z8xmzGxkzvazo237zGxz/KUGAABIRlItbHdI2ijp+rkbzew0SedKeqmksyV9ycx6zKxH0hclvUnSaZLeFT0WAAAg8xK50oG7/1iSzGzhXedI2unuv5N0j5ntk3RmdN8+d787et7O6LF3xVNiAACA5IQ2hq0o6f45tx+Iti22HQAAIPO61sJmZt+V9PwGd33S3a/s4utukrRJkvr6+jQ1NdWtlzriwIEDsbxOGlAXVdRDXat1US5XJCmT9cd+UUddVFEPddRFc10LbO7+xjaeVpJ08pzbJ0Xb1GT7wtfdLmm7JA0NDfnw8HAbxWjN1NSU4nidNKAuqqiHulbqYmK6pHt+dbsOzh7WJ286rLGRgUxd9Jv9oo66qKIe6qiL5kLrEr1K0rlm9jQzWyPpFEm3SLpV0ilmtsbMjlF1YsJVCZYTQIdNTJc0vmuvDs4eliSVyhWN79qriemG380AIFeSWtbjbWb2gKR1kq4xs0lJcvc7JV2m6mSCayV9wN1n3f1JSR+UNCnpx5Iuix4LICO2Tc6ocmh23rbKoVltm5xJqEQAEI6kZoleIemKRe67UNKFDbZ/S9K3ulw0AAnZH41dW+52AMiT0LpEAeRUf2+hpe0AkCcENgBBGBsZUGFVz7xthVU9GhsZSKhEABCORLpEAWCh2mzQbZMz2l+uqL+3kLlZogDQLgIbgGCMDhYJaADQAF2iAAAAgSOwAQAABI7ABgAAEDgCGwAAQOAIbAAAAIEjsAEAAASOwAYAABA4AhsAAEDgCGwAAACBI7ABAAAEjsAGAAAQOAIbAABA4AhsAAAAgSOwAQAABI7ABgAAEDhz96TL0DVm9s+S7o3hpU6Q9IsYXicNqIsq6qGOuqijLuqoiyrqoY66kF7o7s9rdEemA1tczGyPuw8lXY4QUBdV1EMddVFHXdRRF1XUQx110RxdogAAAIEjsAEAAASOwNYZ25MuQECoiyrqoY66qKMu6qiLKuqhjrpogjFsAAAAgaOFDQAAIHAEthUws7PNbMbM9pnZ5qTLEyczu9jMHjGzO+ZsO97MvmNmP43+f06SZYyLmZ1sZt8zs7vM7E4z+/Noe+7qw8yebma3mNmPorr4dLR9jZndHB0rl5rZMUmXNQ5m1mNm02Z2dXQ7r/XwczPba2Y/NLM90bbcHR+SZGa9Zna5mf3EzH5sZuvyWBdmNhDtD7V/vzKzD+exLpaLwNYmM+uR9EVJb5J0mqR3mdlpyZYqVl+TdPaCbZslXefup0i6LrqdB09K+oi7nybpNZI+EO0LeayP30k6y91fIel0SWeb2Wsk/Y2kz7r7v5D0mKT3JVjGOP25pB/PuZ3XepCkDe5++pxlG/J4fEjS5yRd6+6nSnqFqvtH7urC3Wei/eF0SWdI+o2kK5TDulguAlv7zpS0z93vdveDknZKOifhMsXG3a+X9OiCzedIuiT6+RJJo7EWKiHu/qC7/yD6+deqnoCLymF9eNWB6Oaq6J9LOkvS5dH2XNSFmZ0k6S2SvhLdNuWwHprI3fFhZs+W9HpJX5Ukdz/o7mXlsC4WeIOkn7n7vaIuFkVga19R0v1zbj8QbcuzPnd/MPr5IUl9SRYmCWa2WtKgpJuV0/qIugF/KOkRSd+R9DNJZXd/MnpIXo6Vv5f0MUmHo9vPVT7rQaqG9m+b2W1mtinalsfjY42kf5b0P6Ku8q+Y2bHKZ13Mda6kHdHPea+LRRHY0BVenX6cqynIZnacpG9K+rC7/2rufXmqD3efjbo5TlK1JfrUhIsUOzN7q6RH3P22pMsSiNe5+ytVHULyATN7/dw7c3R8HC3plZIucvdBSU9oQZdfjupCkhSN4/wjSd9YeF/e6mIpBLb2lSSdPOf2SdG2PHvYzE6UpOj/RxIuT2zMbJWqYe1/u/uuaHNu60OSoq6e70laJ6nXzI6O7srDsbJe0h+Z2c9VHS5xlqpjl/JWD5Ikdy9F/z+i6jilM5XP4+MBSQ+4+83R7ctVDXB5rIuaN0n6gbs/HN3Oc100RWBr362STolmfR2japPuVQmXKWlXSTov+vk8SVcmWJbYRGOTvirpx+7+X+fclbv6MLPnmVlv9HNB0h+oOqbve5LeHj0s83Xh7uPufpK7r1b13LDb3f9UOasHSTKzY83smbWfJf2hpDuUw+PD3R+SdL+ZDUSb3iDpLuWwLuZ4l+rdoVK+66IpFs5dATN7s6rjVHokXezuFyZcpNiY2Q5Jw5JOkPSwpE9JmpB0maQXSLpX0jvdfeHEhMwxs9dJ+r6kvaqPV/qEquPYclUfZvZyVQcK96j6hfAyd/8rM3uRqi1Nx0ualvRv3f13yZU0PmY2LOmj7v7WPNZD9DdfEd08WtLX3f1CM3uucnZ8SJKZna7qRJRjJN0t6T2KjhXlry6OlXSfpBe5++PRtlzuF8tBYAMAAAgcXaIAAACBI7ABAAAEjsAGAAAQOAIbAABA4AhsAAAAgSOwAUgFM/uemY0s2PZhM7uohd/xV2b2xiUeM2VmQw22n29mX2jhtYbM7PPLfXz0nAvM7KOtPAdAPhy99EMAIAg7VF2EdnLOtnNVvV7nksysx93/shsFa8Td90jaE9frAcg2WtgApMXlkt4SXVlEZrZaUr+k75vZRWa2x8zuNLNP155gZj83s78xsx9IeoeZfc3M3h7d95dmdquZ3WFm26MrVtS828x+GN135sKCRFd0+Gb0/FvNbH2Dxwyb2dXRzxeY2cVR693dZvahOY/7pJn9PzP7R0kDc7a/2MyujS6Y/n0zO9XMjo5ebzh6zBYzy82C3UCeEdgApEK02vktql57UKq2rl0WXSD6k+4+JOnlkn4/uuJCzS/d/ZXuvnPBr/yCu7/K3V8mqSDprXPue0Z0Afv/KOniBsX5nKTPuvurJP2xqivXL+VUSSOqXkfzU2a2yszOiP6O0yW9WdKr5jx+u6Q/c/czJH1U0pfc/UlJ50u6KOraPVvSpwUg8+gSBZAmtW7RK6P/3xdtf6eZbVL1nHaipNMk3R7dd+kiv2uDmX1M0jNUvVTUnZL+z5zXkbtfb2bPql0fdY43SjptTqPcs8zsOHc/0KTs10SXofqdmT0iqU/Sv5J0hbv/RpLM7Kro/+MkvVbSN+a8xtOiMt1pZv9T0tWS1rn7wSavCSAjCGwA0uRKSZ81s1eq2gp2m5mtUbUF6lXu/piZfU3S0+c854mFv8TMni7pS5KG3P1+M7tgwXMWXrNv4e2jJL3G3X/bQtnnXjN0Vs3Pv0dJKketfI2slVSW9HstvD6AFKNLFEBqRC1Y31O1m3JHtPlZqoayx82sT/Uu02Zq4ewXUWvW2xfc/yeSZGavk/R47cLUc3xb0p/VbkQX9G7H9ZJGzaxgZs+U9K8lyd1/JekeM3tH9PvNzF4R/bxR1RbB10v6bw1a/wBkEC1sANJmh6QrVO0Slbv/yMymJf1E0v2SbljqF7h72cy+LOkOSQ9JunXBQ34b/c5Vkt7b4Fd8SNIXzex2Vc+j10v6D63+Ie7+AzO7VNKPJD2yoBx/qupYtf8clWOnmZUkbZX0hqhl8Auqjqc7r9XXBpAuVh2vCwAAgFDRJQoAABA4AhsAAEDgCGwAAACBI7ABAAAEjsAGAAAQOAIbAABA4AhsAAAAgSOwAQAABO7/A/7mOPnoaMQ4AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "Tlw5s4PHK5Fj"
      },
      "source": [
        "## L1-Regularization\n",
        "\n",
        "This section is bonus.\n",
        "\n",
        "In most genetic problems, only a limited number of the tested genes are likely influence any particular attribute.  Hence, we would expect that the weight coefficients in the logistic regression model should be sparse.  That is, they should be zero on any gene that plays no role in the particular attribute of interest.  Genetic analysis commonly imposes sparsity by adding an l1-penalty term.  Read the `sklearn` [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) on the `LogisticRegression` class to see how to set the l1-penalty and the inverse regularization strength, `C`.\n",
        "\n",
        "Using the model selection strategies from the [housing demo](../unit05_lasso/demo2_housing.ipynb), use K-fold cross validation to select an appropriate inverse regularization strength.  \n",
        "* Use 10-fold cross validation \n",
        "* You should select around 20 values of `C`.  It is up to you find a good range.\n",
        "* Make appropriate plots and print out to display your results\n",
        "* How does the accuracy compare to the accuracy achieved without regularization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jELk37RfK5Fk"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz_iouN2K5Fl"
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": []
    }
  ]
}